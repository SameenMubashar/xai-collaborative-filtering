{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43fe4339",
   "metadata": {},
   "source": [
    "# SVD Model Training and Evaluation\n",
    "\n",
    "This notebook trains a Singular Value Decomposition (SVD) model using the Surprise library on the preprocessed MovieLens 1M dataset.\n",
    "\n",
    "**Dataset:** MovieLens 1M (preprocessed)\n",
    "**Algorithm:** SVD (Singular Value Decomposition)\n",
    "**Evaluation Metrics:** RMSE, MAE\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions for Google Colab\n",
    "\n",
    "1. Upload your preprocessed files to Google Drive:\n",
    "   - `train_set.csv`\n",
    "   - `test_set.csv`\n",
    "   - `preprocessing_metadata.json`\n",
    "\n",
    "2. Mount your Google Drive when prompted\n",
    "3. Update the `DATA_PATH` variable below to point to your files\n",
    "4. Run all cells sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4570d41b",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355117a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install scikit-surprise library\n",
    "!pip install scikit-surprise pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf59dae",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ffafa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Surprise library imports\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import cross_validate, GridSearchCV\n",
    "from surprise import accuracy\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c068aec8",
   "metadata": {},
   "source": [
    "## 3. Mount Google Drive (For Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ebd94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if using Google Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Update this path to where you uploaded your preprocessed data\n",
    "DATA_PATH = '/content/drive/MyDrive/xai-collaborative-filtering/data/processed/'\n",
    "\n",
    "# If running locally, comment out the above and use:\n",
    "# DATA_PATH = '../data/processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec420b0",
   "metadata": {},
   "source": [
    "## 4. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb145c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test sets\n",
    "print(\"Loading preprocessed data...\")\n",
    "\n",
    "train_df = pd.read_csv(DATA_PATH + 'train_set.csv')\n",
    "test_df = pd.read_csv(DATA_PATH + 'test_set.csv')\n",
    "\n",
    "# Load metadata\n",
    "with open(DATA_PATH + 'preprocessing_metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"\\nTrain set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"\\nTrain ratings: {len(train_df):,}\")\n",
    "print(f\"Test ratings: {len(test_df):,}\")\n",
    "print(f\"\\nMetadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5fa47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of train data\n",
    "print(\"Train set sample:\")\n",
    "display(train_df.head(10))\n",
    "\n",
    "print(\"\\nTrain set statistics:\")\n",
    "display(train_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88292a1",
   "metadata": {},
   "source": [
    "## 5. Prepare Data for Surprise Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e12a0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rating scale\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# Load data into Surprise format\n",
    "print(\"Converting data to Surprise format...\")\n",
    "\n",
    "# Create train dataset\n",
    "train_data = Dataset.load_from_df(\n",
    "    train_df[['user_id', 'movie_id', 'rating']], \n",
    "    reader\n",
    ")\n",
    "trainset = train_data.build_full_trainset()\n",
    "\n",
    "# Create test dataset (list of tuples format)\n",
    "testset = [(row['user_id'], row['movie_id'], row['rating']) \n",
    "           for _, row in test_df.iterrows()]\n",
    "\n",
    "print(f\"\\nTrainset statistics:\")\n",
    "print(f\"  Number of users: {trainset.n_users}\")\n",
    "print(f\"  Number of items: {trainset.n_items}\")\n",
    "print(f\"  Number of ratings: {trainset.n_ratings}\")\n",
    "print(f\"  Rating scale: {trainset.rating_scale}\")\n",
    "print(f\"\\nTestset size: {len(testset):,} ratings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4869eb91",
   "metadata": {},
   "source": [
    "## 6. Baseline Model - Global Mean\n",
    "\n",
    "Let's establish a baseline by predicting the global mean rating for all test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66331531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate baseline metrics using global mean\n",
    "global_mean = train_df['rating'].mean()\n",
    "\n",
    "# Calculate RMSE and MAE for baseline\n",
    "baseline_predictions = [global_mean] * len(test_df)\n",
    "baseline_rmse = np.sqrt(np.mean((test_df['rating'] - baseline_predictions) ** 2))\n",
    "baseline_mae = np.mean(np.abs(test_df['rating'] - baseline_predictions))\n",
    "\n",
    "print(f\"Baseline Model (Global Mean = {global_mean:.3f})\")\n",
    "print(f\"  RMSE: {baseline_rmse:.4f}\")\n",
    "print(f\"  MAE:  {baseline_mae:.4f}\")\n",
    "print(\"\\nThis is our baseline to beat!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4478a91d",
   "metadata": {},
   "source": [
    "## 7. Train SVD Model with Default Parameters\n",
    "\n",
    "First, let's train an SVD model with default parameters to get a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c0d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SVD with default parameters\n",
    "print(\"Training SVD model with default parameters...\\n\")\n",
    "\n",
    "svd_default = SVD(random_state=42, verbose=True)\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "svd_default.fit(trainset)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024945f8",
   "metadata": {},
   "source": [
    "## 8. Evaluate Default SVD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef2038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "print(\"Evaluating SVD model on test set...\\n\")\n",
    "\n",
    "predictions = svd_default.test(testset)\n",
    "\n",
    "# Calculate RMSE and MAE\n",
    "rmse = accuracy.rmse(predictions, verbose=True)\n",
    "mae = accuracy.mae(predictions, verbose=True)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"SVD Model Performance (Default Parameters)\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "print(f\"\\nImprovement over baseline:\")\n",
    "print(f\"  RMSE: {baseline_rmse - rmse:.4f} ({((baseline_rmse - rmse) / baseline_rmse * 100):.2f}% reduction)\")\n",
    "print(f\"  MAE:  {baseline_mae - mae:.4f} ({((baseline_mae - mae) / baseline_mae * 100):.2f}% reduction)\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323b40cd",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning with Grid Search\n",
    "\n",
    "Let's optimize the SVD model by tuning hyperparameters:\n",
    "- `n_factors`: Number of latent factors\n",
    "- `n_epochs`: Number of training iterations\n",
    "- `lr_all`: Learning rate for all parameters\n",
    "- `reg_all`: Regularization term for all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f660963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_factors': [50, 100, 150],\n",
    "    'n_epochs': [20, 30, 40],\n",
    "    'lr_all': [0.005, 0.01],\n",
    "    'reg_all': [0.02, 0.1]\n",
    "}\n",
    "\n",
    "print(\"Starting Grid Search for optimal hyperparameters...\")\n",
    "print(f\"Parameter grid: {param_grid}\")\n",
    "print(f\"Total combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "print(\"\\nThis may take several minutes...\\n\")\n",
    "\n",
    "# Perform grid search with 3-fold cross-validation\n",
    "gs = GridSearchCV(\n",
    "    SVD,\n",
    "    param_grid,\n",
    "    measures=['rmse', 'mae'],\n",
    "    cv=3,\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    joblib_verbose=2\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "gs.fit(train_data)\n",
    "grid_search_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nGrid search completed in {grid_search_time:.2f} seconds ({grid_search_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378d4925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best parameters and scores\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Grid Search Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nBest RMSE: {gs.best_score['rmse']:.4f}\")\n",
    "print(f\"Best parameters (RMSE):\")\n",
    "for param, value in gs.best_params['rmse'].items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest MAE: {gs.best_score['mae']:.4f}\")\n",
    "print(f\"Best parameters (MAE):\")\n",
    "for param, value in gs.best_params['mae'].items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9374a5",
   "metadata": {},
   "source": [
    "## 10. Train Optimized SVD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f86a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVD with best parameters (optimizing for RMSE)\n",
    "best_params = gs.best_params['rmse']\n",
    "\n",
    "print(\"Training optimized SVD model...\")\n",
    "print(f\"Parameters: {best_params}\\n\")\n",
    "\n",
    "svd_optimized = SVD(\n",
    "    n_factors=best_params['n_factors'],\n",
    "    n_epochs=best_params['n_epochs'],\n",
    "    lr_all=best_params['lr_all'],\n",
    "    reg_all=best_params['reg_all'],\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "svd_optimized.fit(trainset)\n",
    "optimized_training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining completed in {optimized_training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cb8d45",
   "metadata": {},
   "source": [
    "## 11. Evaluate Optimized SVD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a7ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "print(\"Evaluating optimized SVD model on test set...\\n\")\n",
    "\n",
    "predictions_optimized = svd_optimized.test(testset)\n",
    "\n",
    "# Calculate RMSE and MAE\n",
    "rmse_optimized = accuracy.rmse(predictions_optimized, verbose=True)\n",
    "mae_optimized = accuracy.mae(predictions_optimized, verbose=True)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"SVD Model Performance (Optimized Parameters)\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"RMSE: {rmse_optimized:.4f}\")\n",
    "print(f\"MAE:  {mae_optimized:.4f}\")\n",
    "print(f\"Training Time: {optimized_training_time:.2f} seconds\")\n",
    "print(f\"\\nImprovement over baseline:\")\n",
    "print(f\"  RMSE: {baseline_rmse - rmse_optimized:.4f} ({((baseline_rmse - rmse_optimized) / baseline_rmse * 100):.2f}% reduction)\")\n",
    "print(f\"  MAE:  {baseline_mae - mae_optimized:.4f} ({((baseline_mae - mae_optimized) / baseline_mae * 100):.2f}% reduction)\")\n",
    "print(f\"\\nImprovement over default SVD:\")\n",
    "print(f\"  RMSE: {rmse - rmse_optimized:.4f} ({((rmse - rmse_optimized) / rmse * 100):.2f}% reduction)\")\n",
    "print(f\"  MAE:  {mae - mae_optimized:.4f} ({((mae - mae_optimized) / mae * 100):.2f}% reduction)\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0ebdcf",
   "metadata": {},
   "source": [
    "## 12. Analyze Prediction Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c1ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions and actual ratings\n",
    "y_true = [pred.r_ui for pred in predictions_optimized]\n",
    "y_pred = [pred.est for pred in predictions_optimized]\n",
    "errors = np.array(y_true) - np.array(y_pred)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Error distribution\n",
    "axes[0, 0].hist(errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "axes[0, 0].set_xlabel('Prediction Error (Actual - Predicted)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title(f'Error Distribution (Mean: {np.mean(errors):.4f}, Std: {np.std(errors):.4f})')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Actual vs Predicted scatter plot\n",
    "axes[0, 1].scatter(y_true, y_pred, alpha=0.3, s=10)\n",
    "axes[0, 1].plot([1, 5], [1, 5], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 1].set_xlabel('Actual Rating')\n",
    "axes[0, 1].set_ylabel('Predicted Rating')\n",
    "axes[0, 1].set_title('Actual vs Predicted Ratings')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Error by actual rating\n",
    "rating_groups = pd.DataFrame({'actual': y_true, 'error': np.abs(errors)})\n",
    "rating_groups.groupby('actual')['error'].mean().plot(kind='bar', ax=axes[1, 0], color='steelblue')\n",
    "axes[1, 0].set_xlabel('Actual Rating')\n",
    "axes[1, 0].set_ylabel('Mean Absolute Error')\n",
    "axes[1, 0].set_title('MAE by Actual Rating')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 4. Cumulative error distribution\n",
    "sorted_abs_errors = np.sort(np.abs(errors))\n",
    "cumulative = np.arange(1, len(sorted_abs_errors) + 1) / len(sorted_abs_errors) * 100\n",
    "axes[1, 1].plot(sorted_abs_errors, cumulative, linewidth=2)\n",
    "axes[1, 1].axvline(0.5, color='red', linestyle='--', alpha=0.5, label='0.5 error threshold')\n",
    "axes[1, 1].axvline(1.0, color='orange', linestyle='--', alpha=0.5, label='1.0 error threshold')\n",
    "axes[1, 1].set_xlabel('Absolute Error')\n",
    "axes[1, 1].set_ylabel('Cumulative Percentage (%)')\n",
    "axes[1, 1].set_title('Cumulative Error Distribution')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print error statistics\n",
    "print(\"\\nError Analysis:\")\n",
    "print(f\"  Mean Error: {np.mean(errors):.4f}\")\n",
    "print(f\"  Std Error: {np.std(errors):.4f}\")\n",
    "print(f\"  Min Error: {np.min(errors):.4f}\")\n",
    "print(f\"  Max Error: {np.max(errors):.4f}\")\n",
    "print(f\"  Median Absolute Error: {np.median(np.abs(errors)):.4f}\")\n",
    "print(f\"  % predictions within ±0.5: {(np.abs(errors) <= 0.5).mean() * 100:.2f}%\")\n",
    "print(f\"  % predictions within ±1.0: {(np.abs(errors) <= 1.0).mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf0728d",
   "metadata": {},
   "source": [
    "## 13. Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Baseline (Global Mean)', 'SVD (Default)', 'SVD (Optimized)'],\n",
    "    'RMSE': [baseline_rmse, rmse, rmse_optimized],\n",
    "    'MAE': [baseline_mae, mae, mae_optimized],\n",
    "    'Training Time (s)': [0, training_time, optimized_training_time]\n",
    "}).round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "display(comparison_df)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE comparison\n",
    "axes[0].bar(comparison_df['Model'], comparison_df['RMSE'], color=['gray', 'steelblue', 'darkgreen'])\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_title('RMSE Comparison')\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(comparison_df['RMSE']):\n",
    "    axes[0].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# MAE comparison\n",
    "axes[1].bar(comparison_df['Model'], comparison_df['MAE'], color=['gray', 'steelblue', 'darkgreen'])\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('MAE Comparison')\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(comparison_df['MAE']):\n",
    "    axes[1].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe74df",
   "metadata": {},
   "source": [
    "## 14. Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae819dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the optimized model\n",
    "model_path = DATA_PATH + 'svd_optimized_model.pkl'\n",
    "\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(svd_optimized, f)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_type': 'SVD',\n",
    "    'library': 'Surprise',\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'parameters': best_params,\n",
    "    'performance': {\n",
    "        'rmse': float(rmse_optimized),\n",
    "        'mae': float(mae_optimized),\n",
    "        'training_time': float(optimized_training_time)\n",
    "    },\n",
    "    'dataset': {\n",
    "        'train_size': len(train_df),\n",
    "        'test_size': len(test_df),\n",
    "        'n_users': trainset.n_users,\n",
    "        'n_items': trainset.n_items\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = DATA_PATH + 'svd_model_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "print(f\"Model metadata saved to: {metadata_path}\")\n",
    "print(\"\\nModel training and evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affc917c",
   "metadata": {},
   "source": [
    "## 15. Example: Making Predictions\n",
    "\n",
    "Let's see how to use the trained model to make predictions for specific users and movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23816c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some random test samples\n",
    "sample_predictions = np.random.choice(predictions_optimized, size=10, replace=False)\n",
    "\n",
    "print(\"Sample Predictions from Test Set:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'User ID':<10} {'Movie ID':<10} {'Actual':<10} {'Predicted':<12} {'Error':<10}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for pred in sample_predictions:\n",
    "    error = pred.r_ui - pred.est\n",
    "    print(f\"{pred.uid:<10} {pred.iid:<10} {pred.r_ui:<10.1f} {pred.est:<12.3f} {error:<10.3f}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2539a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get top-N recommendations for a user\n",
    "def get_top_n_recommendations(model, user_id, n=10, items_to_predict=None):\n",
    "    \"\"\"\n",
    "    Get top N movie recommendations for a specific user.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained Surprise model\n",
    "    - user_id: User ID\n",
    "    - n: Number of recommendations to return\n",
    "    - items_to_predict: List of movie IDs to predict (if None, predict for all items)\n",
    "    \"\"\"\n",
    "    # Get list of all movies if not specified\n",
    "    if items_to_predict is None:\n",
    "        items_to_predict = train_df['movie_id'].unique()\n",
    "    \n",
    "    # Predict ratings for all movies\n",
    "    predictions = []\n",
    "    for item_id in items_to_predict:\n",
    "        pred = model.predict(user_id, item_id)\n",
    "        predictions.append((item_id, pred.est))\n",
    "    \n",
    "    # Sort by predicted rating\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top N\n",
    "    return predictions[:n]\n",
    "\n",
    "# Example: Get recommendations for a random user\n",
    "sample_user = train_df['user_id'].sample(1).values[0]\n",
    "print(f\"\\nTop 10 movie recommendations for User {sample_user}:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Rank':<6} {'Movie ID':<12} {'Predicted Rating':<20}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get movies user hasn't rated yet\n",
    "user_rated_movies = train_df[train_df['user_id'] == sample_user]['movie_id'].values\n",
    "all_movies = train_df['movie_id'].unique()\n",
    "movies_to_predict = [m for m in all_movies if m not in user_rated_movies]\n",
    "\n",
    "recommendations = get_top_n_recommendations(\n",
    "    svd_optimized, \n",
    "    sample_user, \n",
    "    n=10, \n",
    "    items_to_predict=movies_to_predict[:100]  # Predict on subset for speed\n",
    ")\n",
    "\n",
    "for rank, (movie_id, rating) in enumerate(recommendations, 1):\n",
    "    print(f\"{rank:<6} {movie_id:<12} {rating:<20.3f}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (XAI-CF)",
   "language": "python",
   "name": "xai-cf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
